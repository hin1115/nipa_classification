{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import PIL.Image as pilimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if exist error : use below command\n",
    "# pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory for save traing weight\n",
    "PATH = './ckpt_folder/'\n",
    "\n",
    "if not os.path.isdir(PATH):\n",
    "    os.mkdir(PATH)\n",
    "\n",
    "# define the hyper parameters for training\n",
    "nb_batch = 8\n",
    "nb_epochs = 50\n",
    "lr = 1e-4\n",
    "validation_split = 0.2\n",
    "random_seed= 42\n",
    "shuffle_dataset = True # shuffle dataset from training data for splitted dataset : entire_train = train + valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset loader ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, data_dir='train/', class_num_1=13, class_num_2=20):\n",
    "        self.dir = data_dir\n",
    "        self.data_pose = data_dir + 'train.tsv'\n",
    "        # Need zero class >> plus one part\n",
    "        self.class_num_1 = class_num_1 + 1 # number of first label\n",
    "        self.class_num_2 = class_num_2 + 1 # number of second label\n",
    "        self.enc = OneHotEncoder\n",
    "        \n",
    "        dataset = []\n",
    "        for line in open(self.data_pose,'r'):\n",
    "            spl = line.strip().split('\\t')\n",
    "            dataset.append([spl[0], spl[1], spl[2]])\n",
    "        \n",
    "        dataset_np = np.array(dataset)\n",
    "        \n",
    "        self.img_name = dataset_np[:,0]\n",
    "        self.cls_1 = dataset_np[:,1].astype(np.int)\n",
    "        self.cls_2 = dataset_np[:,2].astype(np.int)\n",
    "        \n",
    "    # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.img_name)\n",
    "\n",
    "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        # image part\n",
    "        img_name = self.dir + self.img_name[idx]\n",
    "        img_data = pilimg.open(img_name)\n",
    "        img_arr = np.array(img_data) / 255.\n",
    "        # transform img_arr from [height, width, depth] to [depth, height, width]\n",
    "        x_input = torch.FloatTensor( np.transpose(img_arr, (2, 0, 1)) )\n",
    "        \n",
    "        # label - 1 part\n",
    "        y1_lab_onehot = np.zeros(shape=(self.class_num_1,), dtype=np.int8)\n",
    "        y1_lab_onehot[self.cls_1[idx]] = 1\n",
    "        \n",
    "        # label - 2 part\n",
    "        y2_lab_onehot = np.zeros(shape=(self.class_num_2,), dtype=np.int8)\n",
    "        y2_lab_onehot[self.cls_2[idx]] = 1\n",
    "        \n",
    "        x = torch.FloatTensor(x_input)\n",
    "        y1 = torch.FloatTensor(y1_lab_onehot)\n",
    "        y2 = torch.FloatTensor(y2_lab_onehot)\n",
    "        return x, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and shuffling reference site : \n",
    "# https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data loader\n",
    "dataset_train = CustomDataset()\n",
    "dataset_size = len(dataset_train)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "#train_loader, valid_loader = torch.utils.data.random_split(dataset_train.dataset, batch_size=nb_batch, sampler=[train_sampler, valid_sampler])\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=nb_batch, sampler=train_sampler)\n",
    "valid_loader = DataLoader(dataset_train, batch_size=nb_batch, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the models for training ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _res_block(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(_res_block, self).__init__()\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block_1(x)\n",
    "        out = self.block_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_1 = nn.BatchNorm2d(64)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv_2 = _res_block(in_channels=64)        \n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_3 = nn.BatchNorm2d(128)\n",
    "        self.relu_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv_4 = _res_block(in_channels=128)\n",
    "        \n",
    "        self.conv_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_5 = nn.BatchNorm2d(256)\n",
    "        self.relu_5 = nn.ReLU()\n",
    "        \n",
    "        self.conv_6 = _res_block(in_channels=256)\n",
    "        \n",
    "        self.conv_7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_7 = nn.BatchNorm2d(512)\n",
    "        self.relu_7 = nn.ReLU()\n",
    "        \n",
    "        self.conv_8 = _res_block(in_channels=512)\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(512, 14)\n",
    "        self.fc_2 = nn.Linear(512, 21)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # stage - 1\n",
    "        out = self.bn_1(self.conv_1(x))\n",
    "        out = self.relu_1(out)\n",
    "        out = self.conv_2(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        # stage - 2\n",
    "        out = self.bn_3(self.conv_3(out))\n",
    "        out = self.relu_3(out)\n",
    "        out = self.conv_4(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        # stage - 3\n",
    "        out = self.bn_5(self.conv_5(out))\n",
    "        out = self.relu_5(out)\n",
    "        out = self.conv_6(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        # stage - 4\n",
    "        out = self.bn_7(self.conv_7(out))\n",
    "        out = self.relu_7(out)\n",
    "        out = self.conv_8(out)\n",
    "        \n",
    "        # glocal average pooling \n",
    "        out = self.gap(out)\n",
    "        \n",
    "        # fully connected layer and soft-max\n",
    "        out_flat = out.view(out.size(0), -1)\n",
    "        out_1 = self.fc_1(out_flat)\n",
    "        out_2 = self.fc_2(out_flat)\n",
    "        \n",
    "        return self.softmax(out_1), self.softmax(out_2)\n",
    "        #return out_1, out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE_one_hot_to_num(input_onehot):\n",
    "    _, input_num = input_onehot.max(dim=1)\n",
    "    return input_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Net().to(device)\n",
    "#criterion = F.mse_loss\n",
    "#criterion = F.cross_entropy # >> Not work\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5,10,15,20,25,30,35,40,45], gamma= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy one-hot vector form code reference : \n",
    "# https://discuss.pytorch.org/t/cross-entropy-with-one-hot-targets/13580/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "    Args:\n",
    "         pred: predictions for neural network\n",
    "         targets: targets, can be soft\n",
    "         size_average: if false, sum is returned instead of mean\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
    "        input = torch.autograd.Variable(out, requires_grad=True)\n",
    "\n",
    "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "        target = torch.autograd.Variable(y1)\n",
    "        loss = cross_entropy(input, target)\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    logsoftmax = nn.LogSoftmax()\n",
    "    if size_average:\n",
    "        return torch.mean(torch.sum(-target * logsoftmax(pred), dim=1))\n",
    "    else:\n",
    "        return torch.sum(torch.sum(-target * logsoftmax(pred), dim=1))\n",
    "    \n",
    "criterion = cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training~!! =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/home/hoyin/anaconda3-1/envs/bear_solaris/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t iteration num : 0 // train loss value : 5.680109\n",
      "\t iteration num : 100 // train loss value : 5.390731\n",
      "\t iteration num : 200 // train loss value : 5.055564\n",
      "\t iteration num : 300 // train loss value : 4.721043\n",
      "\t iteration num : 400 // train loss value : 5.320446\n",
      "\t iteration num : 500 // train loss value : 4.916174\n",
      "\t iteration num : 600 // train loss value : 5.003072\n",
      "\t iteration num : 700 // train loss value : 5.058442\n",
      "\t iteration num : 800 // train loss value : 4.768394\n",
      "\t iteration num : 900 // train loss value : 5.383141\n",
      "\t iteration num : 1000 // train loss value : 5.067809\n",
      "\t iteration num : 1100 // train loss value : 4.886039\n",
      "\t iteration num : 1200 // train loss value : 5.104815\n",
      "\t iteration num : 1300 // train loss value : 4.999990\n",
      "\t iteration num : 1400 // train loss value : 4.749073\n",
      "\t iteration num : 1500 // train loss value : 5.010344\n",
      "Epoch num : 1 // train loss value : 5.120950\n",
      "validation loss : 4.966242\n",
      "validation accuracy : case - 1 : 58.812500 // case-2 : 36.406250\n",
      "\t iteration num : 0 // train loss value : 5.261630\n",
      "\t iteration num : 100 // train loss value : 5.064059\n",
      "\t iteration num : 200 // train loss value : 4.671527\n",
      "\t iteration num : 300 // train loss value : 4.640367\n",
      "\t iteration num : 400 // train loss value : 5.114807\n",
      "\t iteration num : 500 // train loss value : 4.751808\n",
      "\t iteration num : 600 // train loss value : 4.752809\n",
      "\t iteration num : 700 // train loss value : 4.768040\n",
      "\t iteration num : 800 // train loss value : 4.583630\n",
      "\t iteration num : 900 // train loss value : 4.978438\n",
      "\t iteration num : 1000 // train loss value : 4.795274\n",
      "\t iteration num : 1100 // train loss value : 4.662504\n",
      "\t iteration num : 1200 // train loss value : 5.064500\n",
      "\t iteration num : 1300 // train loss value : 4.812616\n",
      "\t iteration num : 1400 // train loss value : 4.694300\n",
      "\t iteration num : 1500 // train loss value : 5.037333\n",
      "Epoch num : 2 // train loss value : 4.685813\n",
      "validation loss : 4.780794\n",
      "validation accuracy : case - 1 : 61.531250 // case-2 : 51.156250\n",
      "\t iteration num : 0 // train loss value : 5.178082\n",
      "\t iteration num : 100 // train loss value : 5.155032\n",
      "\t iteration num : 200 // train loss value : 4.575063\n",
      "\t iteration num : 300 // train loss value : 4.942190\n",
      "\t iteration num : 400 // train loss value : 4.924241\n",
      "\t iteration num : 500 // train loss value : 5.083890\n",
      "\t iteration num : 600 // train loss value : 4.619119\n",
      "\t iteration num : 700 // train loss value : 4.791813\n",
      "\t iteration num : 800 // train loss value : 4.836035\n",
      "\t iteration num : 900 // train loss value : 4.599816\n",
      "\t iteration num : 1000 // train loss value : 5.217737\n",
      "\t iteration num : 1100 // train loss value : 5.074674\n",
      "\t iteration num : 1200 // train loss value : 4.818558\n",
      "\t iteration num : 1300 // train loss value : 5.607476\n",
      "\t iteration num : 1400 // train loss value : 4.973931\n",
      "\t iteration num : 1500 // train loss value : 4.768795\n",
      "Epoch num : 3 // train loss value : 5.270488\n",
      "validation loss : 4.728847\n",
      "validation accuracy : case - 1 : 68.812500 // case-2 : 50.656250\n",
      "\t iteration num : 0 // train loss value : 4.879872\n",
      "\t iteration num : 100 // train loss value : 4.703604\n",
      "\t iteration num : 200 // train loss value : 4.689964\n",
      "\t iteration num : 300 // train loss value : 4.863235\n",
      "\t iteration num : 400 // train loss value : 4.485206\n",
      "\t iteration num : 500 // train loss value : 4.868677\n",
      "\t iteration num : 600 // train loss value : 4.455300\n",
      "\t iteration num : 700 // train loss value : 4.758936\n",
      "\t iteration num : 800 // train loss value : 4.897979\n",
      "\t iteration num : 900 // train loss value : 5.017726\n",
      "\t iteration num : 1000 // train loss value : 4.421004\n",
      "\t iteration num : 1100 // train loss value : 4.656145\n",
      "\t iteration num : 1200 // train loss value : 4.515281\n",
      "\t iteration num : 1300 // train loss value : 4.597257\n",
      "\t iteration num : 1400 // train loss value : 4.499104\n",
      "\t iteration num : 1500 // train loss value : 5.176477\n",
      "Epoch num : 4 // train loss value : 4.592990\n",
      "validation loss : 4.476823\n",
      "validation accuracy : case - 1 : 78.062500 // case-2 : 65.093750\n",
      "\t iteration num : 0 // train loss value : 4.471431\n",
      "\t iteration num : 100 // train loss value : 4.406139\n",
      "\t iteration num : 200 // train loss value : 4.425354\n",
      "\t iteration num : 300 // train loss value : 4.433270\n",
      "\t iteration num : 400 // train loss value : 4.785624\n",
      "\t iteration num : 500 // train loss value : 4.869803\n",
      "\t iteration num : 600 // train loss value : 4.742622\n",
      "\t iteration num : 700 // train loss value : 4.274812\n",
      "\t iteration num : 800 // train loss value : 4.733258\n",
      "\t iteration num : 900 // train loss value : 4.264199\n",
      "\t iteration num : 1000 // train loss value : 4.309322\n",
      "\t iteration num : 1100 // train loss value : 4.795265\n",
      "\t iteration num : 1200 // train loss value : 4.327601\n",
      "\t iteration num : 1300 // train loss value : 4.533601\n",
      "\t iteration num : 1400 // train loss value : 4.556945\n",
      "\t iteration num : 1500 // train loss value : 4.420059\n",
      "Epoch num : 5 // train loss value : 4.433656\n",
      "validation loss : 4.464838\n",
      "validation accuracy : case - 1 : 78.187500 // case-2 : 65.812500\n",
      "\t iteration num : 0 // train loss value : 4.371556\n",
      "\t iteration num : 100 // train loss value : 4.488719\n",
      "\t iteration num : 200 // train loss value : 4.631510\n",
      "\t iteration num : 300 // train loss value : 4.598432\n",
      "\t iteration num : 400 // train loss value : 4.300399\n",
      "\t iteration num : 500 // train loss value : 4.772096\n",
      "\t iteration num : 600 // train loss value : 4.520628\n",
      "\t iteration num : 700 // train loss value : 4.278505\n",
      "\t iteration num : 800 // train loss value : 4.292404\n",
      "\t iteration num : 900 // train loss value : 4.434848\n",
      "\t iteration num : 1000 // train loss value : 4.458117\n",
      "\t iteration num : 1100 // train loss value : 4.764688\n",
      "\t iteration num : 1200 // train loss value : 4.456624\n",
      "\t iteration num : 1300 // train loss value : 4.812559\n",
      "\t iteration num : 1400 // train loss value : 4.972611\n",
      "\t iteration num : 1500 // train loss value : 4.429753\n",
      "Epoch num : 6 // train loss value : 4.711460\n",
      "validation loss : 4.415207\n",
      "validation accuracy : case - 1 : 81.375000 // case-2 : 66.468750\n",
      "\t iteration num : 0 // train loss value : 4.344240\n",
      "\t iteration num : 100 // train loss value : 4.691877\n",
      "\t iteration num : 200 // train loss value : 4.301252\n",
      "\t iteration num : 300 // train loss value : 4.800760\n",
      "\t iteration num : 400 // train loss value : 4.189177\n",
      "\t iteration num : 500 // train loss value : 4.888371\n",
      "\t iteration num : 600 // train loss value : 4.554856\n",
      "\t iteration num : 700 // train loss value : 4.388457\n",
      "\t iteration num : 800 // train loss value : 4.311990\n",
      "\t iteration num : 900 // train loss value : 4.134997\n",
      "\t iteration num : 1000 // train loss value : 4.795336\n",
      "\t iteration num : 1100 // train loss value : 4.521560\n",
      "\t iteration num : 1200 // train loss value : 4.082682\n",
      "\t iteration num : 1300 // train loss value : 4.416586\n",
      "\t iteration num : 1400 // train loss value : 4.653161\n",
      "\t iteration num : 1500 // train loss value : 4.563739\n",
      "Epoch num : 7 // train loss value : 4.419354\n",
      "validation loss : 4.329947\n",
      "validation accuracy : case - 1 : 83.968750 // case-2 : 74.812500\n",
      "\t iteration num : 0 // train loss value : 4.317651\n",
      "\t iteration num : 100 // train loss value : 4.691471\n",
      "\t iteration num : 200 // train loss value : 4.399035\n",
      "\t iteration num : 300 // train loss value : 4.436459\n",
      "\t iteration num : 400 // train loss value : 4.516000\n",
      "\t iteration num : 500 // train loss value : 4.513585\n",
      "\t iteration num : 600 // train loss value : 4.489585\n",
      "\t iteration num : 700 // train loss value : 4.555456\n",
      "\t iteration num : 800 // train loss value : 4.388852\n",
      "\t iteration num : 900 // train loss value : 4.304759\n",
      "\t iteration num : 1000 // train loss value : 4.195585\n",
      "\t iteration num : 1100 // train loss value : 4.040282\n",
      "\t iteration num : 1200 // train loss value : 4.483697\n",
      "\t iteration num : 1300 // train loss value : 4.352109\n",
      "\t iteration num : 1400 // train loss value : 4.920740\n",
      "\t iteration num : 1500 // train loss value : 4.409298\n",
      "Epoch num : 8 // train loss value : 4.143344\n",
      "validation loss : 4.253105\n",
      "validation accuracy : case - 1 : 87.937500 // case-2 : 78.312500\n",
      "\t iteration num : 0 // train loss value : 4.339941\n",
      "\t iteration num : 100 // train loss value : 4.412629\n",
      "\t iteration num : 200 // train loss value : 4.280955\n",
      "\t iteration num : 300 // train loss value : 4.696139\n",
      "\t iteration num : 400 // train loss value : 4.119640\n",
      "\t iteration num : 500 // train loss value : 4.002489\n",
      "\t iteration num : 600 // train loss value : 4.665642\n",
      "\t iteration num : 700 // train loss value : 4.221148\n",
      "\t iteration num : 800 // train loss value : 4.567568\n",
      "\t iteration num : 900 // train loss value : 4.699656\n",
      "\t iteration num : 1000 // train loss value : 4.138238\n",
      "\t iteration num : 1100 // train loss value : 4.534184\n",
      "\t iteration num : 1200 // train loss value : 4.166698\n",
      "\t iteration num : 1300 // train loss value : 4.190778\n",
      "\t iteration num : 1400 // train loss value : 4.126011\n",
      "\t iteration num : 1500 // train loss value : 3.979126\n",
      "Epoch num : 9 // train loss value : 4.294427\n",
      "validation loss : 4.193077\n",
      "validation accuracy : case - 1 : 91.812500 // case-2 : 80.125000\n",
      "\t iteration num : 0 // train loss value : 4.256608\n",
      "\t iteration num : 100 // train loss value : 4.099348\n",
      "\t iteration num : 200 // train loss value : 4.026742\n",
      "\t iteration num : 300 // train loss value : 4.266769\n",
      "\t iteration num : 400 // train loss value : 4.283590\n",
      "\t iteration num : 500 // train loss value : 4.579411\n",
      "\t iteration num : 600 // train loss value : 4.084300\n",
      "\t iteration num : 700 // train loss value : 4.253049\n",
      "\t iteration num : 800 // train loss value : 3.961222\n",
      "\t iteration num : 900 // train loss value : 4.484162\n",
      "\t iteration num : 1000 // train loss value : 4.170272\n",
      "\t iteration num : 1100 // train loss value : 4.604454\n",
      "\t iteration num : 1200 // train loss value : 4.327330\n",
      "\t iteration num : 1300 // train loss value : 4.241836\n",
      "\t iteration num : 1400 // train loss value : 4.039342\n",
      "\t iteration num : 1500 // train loss value : 4.151066\n",
      "Epoch num : 10 // train loss value : 4.071696\n",
      "validation loss : 4.173112\n",
      "validation accuracy : case - 1 : 92.843750 // case-2 : 80.750000\n",
      "\t iteration num : 0 // train loss value : 4.140796\n",
      "\t iteration num : 100 // train loss value : 4.318928\n",
      "\t iteration num : 200 // train loss value : 4.194981\n",
      "\t iteration num : 300 // train loss value : 4.144805\n",
      "\t iteration num : 400 // train loss value : 4.209720\n",
      "\t iteration num : 500 // train loss value : 4.222802\n",
      "\t iteration num : 600 // train loss value : 4.341073\n",
      "\t iteration num : 700 // train loss value : 4.450298\n",
      "\t iteration num : 800 // train loss value : 4.076321\n",
      "\t iteration num : 900 // train loss value : 4.352594\n",
      "\t iteration num : 1000 // train loss value : 4.217885\n",
      "\t iteration num : 1100 // train loss value : 4.163164\n",
      "\t iteration num : 1200 // train loss value : 4.566295\n",
      "\t iteration num : 1300 // train loss value : 4.036688\n",
      "\t iteration num : 1400 // train loss value : 4.228480\n",
      "\t iteration num : 1500 // train loss value : 4.547596\n",
      "Epoch num : 11 // train loss value : 4.084696\n",
      "validation loss : 4.145931\n",
      "validation accuracy : case - 1 : 93.437500 // case-2 : 82.968750\n",
      "\t iteration num : 0 // train loss value : 4.543800\n",
      "\t iteration num : 100 // train loss value : 4.001968\n",
      "\t iteration num : 200 // train loss value : 4.143969\n",
      "\t iteration num : 300 // train loss value : 4.390066\n",
      "\t iteration num : 400 // train loss value : 4.273576\n",
      "\t iteration num : 500 // train loss value : 4.293444\n",
      "\t iteration num : 600 // train loss value : 4.277547\n",
      "\t iteration num : 700 // train loss value : 4.050679\n",
      "\t iteration num : 800 // train loss value : 4.433274\n",
      "\t iteration num : 900 // train loss value : 4.130013\n",
      "\t iteration num : 1000 // train loss value : 3.992988\n",
      "\t iteration num : 1100 // train loss value : 4.150106\n",
      "\t iteration num : 1200 // train loss value : 4.280760\n",
      "\t iteration num : 1300 // train loss value : 4.213387\n",
      "\t iteration num : 1400 // train loss value : 3.938424\n",
      "\t iteration num : 1500 // train loss value : 4.255398\n",
      "Epoch num : 12 // train loss value : 4.292067\n",
      "validation loss : 4.069365\n",
      "validation accuracy : case - 1 : 98.062500 // case-2 : 86.718750\n",
      "\t iteration num : 0 // train loss value : 4.045672\n",
      "\t iteration num : 100 // train loss value : 4.000291\n",
      "\t iteration num : 200 // train loss value : 4.218447\n",
      "\t iteration num : 300 // train loss value : 3.981782\n",
      "\t iteration num : 400 // train loss value : 4.048448\n",
      "\t iteration num : 500 // train loss value : 4.159436\n",
      "\t iteration num : 600 // train loss value : 4.283053\n",
      "\t iteration num : 700 // train loss value : 4.057480\n",
      "\t iteration num : 800 // train loss value : 3.973000\n",
      "\t iteration num : 900 // train loss value : 3.966673\n",
      "\t iteration num : 1000 // train loss value : 4.223539\n",
      "\t iteration num : 1100 // train loss value : 4.188055\n",
      "\t iteration num : 1200 // train loss value : 3.940780\n",
      "\t iteration num : 1300 // train loss value : 3.930744\n",
      "\t iteration num : 1400 // train loss value : 4.026010\n",
      "\t iteration num : 1500 // train loss value : 4.025424\n",
      "Epoch num : 13 // train loss value : 3.960473\n",
      "validation loss : 4.061365\n",
      "validation accuracy : case - 1 : 98.093750 // case-2 : 86.531250\n",
      "\t iteration num : 0 // train loss value : 3.952860\n",
      "\t iteration num : 100 // train loss value : 3.933412\n",
      "\t iteration num : 200 // train loss value : 4.102171\n",
      "\t iteration num : 300 // train loss value : 4.307237\n",
      "\t iteration num : 400 // train loss value : 3.981671\n",
      "\t iteration num : 500 // train loss value : 4.019166\n",
      "\t iteration num : 600 // train loss value : 4.267784\n",
      "\t iteration num : 700 // train loss value : 4.067941\n",
      "\t iteration num : 800 // train loss value : 3.888144\n",
      "\t iteration num : 900 // train loss value : 4.231454\n",
      "\t iteration num : 1000 // train loss value : 3.950487\n",
      "\t iteration num : 1100 // train loss value : 4.399540\n",
      "\t iteration num : 1200 // train loss value : 4.170374\n",
      "\t iteration num : 1300 // train loss value : 4.265630\n",
      "\t iteration num : 1400 // train loss value : 4.000039\n",
      "\t iteration num : 1500 // train loss value : 3.968548\n",
      "Epoch num : 14 // train loss value : 3.899931\n",
      "validation loss : 4.046357\n",
      "validation accuracy : case - 1 : 98.843750 // case-2 : 87.218750\n",
      "\t iteration num : 0 // train loss value : 4.066327\n",
      "\t iteration num : 100 // train loss value : 4.130414\n",
      "\t iteration num : 200 // train loss value : 3.953310\n",
      "\t iteration num : 300 // train loss value : 4.167045\n",
      "\t iteration num : 400 // train loss value : 3.923342\n",
      "\t iteration num : 500 // train loss value : 4.224128\n",
      "\t iteration num : 600 // train loss value : 4.213781\n",
      "\t iteration num : 700 // train loss value : 4.122101\n",
      "\t iteration num : 800 // train loss value : 4.078836\n",
      "\t iteration num : 900 // train loss value : 4.035059\n",
      "\t iteration num : 1000 // train loss value : 4.154678\n",
      "\t iteration num : 1100 // train loss value : 4.122351\n",
      "\t iteration num : 1200 // train loss value : 4.026813\n",
      "\t iteration num : 1300 // train loss value : 3.930012\n",
      "\t iteration num : 1400 // train loss value : 4.003620\n",
      "\t iteration num : 1500 // train loss value : 3.906608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-55537417c359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loss_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t iteration num : {} // train loss value : {:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(\"start training~!! =========================\")\n",
    "\n",
    "# train_loss_check = []\n",
    "# loss_set = []\n",
    "# for epoch in range(nb_epochs):\n",
    "#     scheduler.step() # apply the learning rate scheduler\n",
    "    \n",
    "#     model.train()\n",
    "#     for batch_idx, samples in enumerate(train_loader):\n",
    "#         x_train, y_train1, y_train2 = samples\n",
    "#         data, tar_1, tar_2 = x_train.to(device), y_train1.to(device), y_train2.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output_1, output_2 = model(data)\n",
    "        \n",
    "#         # loss \n",
    "#         loss1 = criterion(target=tar_1, pred=output_1)\n",
    "#         loss2 = criterion(target=tar_2, pred=output_2)\n",
    "        \n",
    "#         loss_val = loss1 + loss2\n",
    "#         loss_val.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss_check.append(loss_val.item())\n",
    "#         if( batch_idx % 100 == 0 ):\n",
    "#             print(\"\\t iteration num : {} // train loss value : {:.6f}\".format(batch_idx, loss_val.item()))\n",
    "        \n",
    "#     print(\"Epoch num : {} // train loss value : {:.6f}\".format(epoch+1, loss_val.item()))\n",
    "#     PATH_ckpt = PATH + \"epoch_\" + str(epoch).zfill(3) + '_ckpt.pth'\n",
    "#     torch.save(model.state_dict(), PATH_ckpt)\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad(): # very very very very important!!!\n",
    "#         test_loss = []\n",
    "#         correct_1 = 0\n",
    "#         correct_2 = 0\n",
    "#         for batch_idx, samples in enumerate(valid_loader):\n",
    "#             x_test, y_test1, y_test2 = samples\n",
    "#             data, tar_1, tar_2 = x_test.to(device), y_test1.to(device), y_test2.to(device)\n",
    "#             output_1, output_2 = model(data)\n",
    "            \n",
    "#             pred_1 = output_1.argmax(dim=1, keepdim=True)\n",
    "#             pred_2 = output_2.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "#             target_1 = tar_1.argmax(dim=1, keepdim=True)\n",
    "#             target_2 = tar_2.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "#             correct_1 += pred_1.eq( target_1.view_as(pred_1)  ).sum().item()\n",
    "#             correct_2 += pred_2.eq( target_2.view_as(pred_2)  ).sum().item()\n",
    "            \n",
    "#             # loss value check\n",
    "#             loss1 = criterion(target=tar_1, pred=output_1)\n",
    "#             loss2 = criterion(target=tar_2, pred=output_2)\n",
    "\n",
    "#             loss_val = loss1 + loss2\n",
    "#             test_loss.append(loss_val.item())\n",
    "            \n",
    "#         corr_1_val = (100. * correct_1) / len(valid_sampler)\n",
    "#         corr_2_val = (100. * correct_2) / len(valid_sampler)\n",
    "\n",
    "#         print(\"validation loss : {:.6f}\".format( np.mean(test_loss) ))\n",
    "#         print(\"validation accuracy : case - 1 : {:.6f} // case-2 : {:.6f}\".format(corr_1_val, corr_2_val) )\n",
    "        \n",
    "#     loss_set.append([ np.mean(train_loss_check), np.mean(test_loss)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev result : validation accuracy : case - 1 : 85.968750 // case-2 : 66.281250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_set_arr = np.array(loss_set)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,1,1)\n",
    "plt.plot(loss_set_arr[:,0])\n",
    "plt.title('train loss ', fontsize=10)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(loss_set_arr[:,1])\n",
    "plt.title('test loss ', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation part script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"dummy/ckpt_folder_0.89_0.74/epoch_009_ckpt.pth\"\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test~!! =========================\n",
      "7.175\n",
      "5.9225\n"
     ]
    }
   ],
   "source": [
    "print(\"start test~!! =========================\")\n",
    "\n",
    "model.eval()\n",
    "f1_score_cls1 = 0\n",
    "f1_score_cls2 = 0\n",
    "\n",
    "with torch.no_grad(): # very very very very important!!!\n",
    "    for batch_idx, samples in enumerate(valid_loader):\n",
    "        x_test, y_test1, y_test2 = samples\n",
    "        data, tar_1, tar_2 = x_test.to(device), y_test1.to(device), y_test2.to(device)\n",
    "        output_1, output_2 = model(data)\n",
    "\n",
    "        pred_1 = output_1.argmax(dim=1, keepdim=True)\n",
    "        pred_2 = output_2.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        target_1 = tar_1.argmax(dim=1, keepdim=True)\n",
    "        target_2 = tar_2.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        #correct_1 += pred_1.eq( target_1.view_as(pred_1)  ).sum().item()\n",
    "        #correct_2 += pred_2.eq( target_2.view_as(pred_2)  ).sum().item()\n",
    "        \n",
    "        for b_idx in range( len(x_test) ):\n",
    "            tar_1_onehot = tar_1.cpu().numpy()[b_idx]\n",
    "            tar_2_onehot = tar_2.cpu().numpy()[b_idx]\n",
    "            \n",
    "            # make prediction result as one-hot vector form\n",
    "            pred_1_idx = pred_1[b_idx]\n",
    "            pred_2_idx = pred_2[b_idx]\n",
    "            \n",
    "            pred_1_onehot = np.zeros(shape=(14,), dtype=np.int8)\n",
    "            pred_2_onehot = np.zeros(shape=(21,), dtype=np.int8)\n",
    "            \n",
    "            pred_1_onehot[pred_1_idx] = 1\n",
    "            pred_2_onehot[pred_2_idx] = 1\n",
    "            \n",
    "            f1_score_cls1 += f1_score( tar_1_onehot, pred_1_onehot )\n",
    "            f1_score_cls2 += f1_score( tar_2_onehot, pred_2_onehot )\n",
    "        \n",
    "    print( f1_score_cls1 / len(valid_loader) ) \n",
    "    print( f1_score_cls2 / len(valid_loader) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2870.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_cls1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
